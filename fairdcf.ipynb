{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13082436,"sourceType":"datasetVersion","datasetId":8285829}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":47.171369,"end_time":"2025-07-21T09:10:02.089164","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-21T09:09:14.917795","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5bb1ac07","cell_type":"code","source":"import torch\n\n# 打印Torch版本\nprint(\"Torch version:\", torch.__version__)\n\n# 打印CUDA版本（如果CUDA可用）\nif torch.cuda.is_available():\n    print(\"CUDA version:\", torch.version.cuda)\nelse:\n    print(\"CUDA is not available.\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2026-01-05T08:43:06.458095Z","iopub.execute_input":"2026-01-05T08:43:06.458717Z","iopub.status.idle":"2026-01-05T08:43:06.463753Z","shell.execute_reply.started":"2026-01-05T08:43:06.458687Z","shell.execute_reply":"2026-01-05T08:43:06.462696Z"},"papermill":{"duration":5.433194,"end_time":"2025-07-21T09:09:25.370210","exception":false,"start_time":"2025-07-21T09:09:19.937016","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Torch version: 2.5.1+cu121\nCUDA version: 12.1\n","output_type":"stream"}],"execution_count":15},{"id":"d5095d7d","cell_type":"code","source":"%cd /kaggle/input/all-datas","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:06.526027Z","iopub.execute_input":"2026-01-05T08:43:06.526276Z","iopub.status.idle":"2026-01-05T08:43:06.530714Z","shell.execute_reply.started":"2026-01-05T08:43:06.526255Z","shell.execute_reply":"2026-01-05T08:43:06.529976Z"},"papermill":{"duration":0.017027,"end_time":"2025-07-21T09:09:25.391894","exception":false,"start_time":"2025-07-21T09:09:25.374867","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/all-datas\n","output_type":"stream"}],"execution_count":16},{"id":"2e7e5dd9-de9c-449a-8e53-3f82067051ac","cell_type":"code","source":"#!pip uninstall -y torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric pyg-lib\n# !pip install -U torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n!pip install -q torch_geometric\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:43:06.541965Z","iopub.execute_input":"2026-01-05T08:43:06.542227Z","iopub.status.idle":"2026-01-05T08:43:12.814680Z","shell.execute_reply.started":"2026-01-05T08:43:06.542199Z","shell.execute_reply":"2026-01-05T08:43:12.813953Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\nRequirement already satisfied: pyg_lib in /usr/local/lib/python3.12/dist-packages (0.4.0+pt25cu121)\nRequirement already satisfied: torch_scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt25cu121)\nRequirement already satisfied: torch_sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt25cu121)\nRequirement already satisfied: torch_cluster in /usr/local/lib/python3.12/dist-packages (1.6.3+pt25cu121)\nRequirement already satisfied: torch_spline_conv in /usr/local/lib/python3.12/dist-packages (1.2.2+pt25cu121)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch_sparse) (1.15.3)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from scipy->torch_sparse) (2.0.2)\n","output_type":"stream"}],"execution_count":18},{"id":"53936d7d","cell_type":"markdown","source":"# util","metadata":{"papermill":{"duration":0.005781,"end_time":"2025-07-21T09:09:42.598430","exception":false,"start_time":"2025-07-21T09:09:42.592649","status":"completed"},"tags":[]}},{"id":"4ce3dbfb","cell_type":"code","source":"from torch_geometric.utils import add_remaining_self_loops, degree\nfrom torch_scatter import scatter\nimport random\nimport torch\nimport os\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\nimport pandas as pd\n\n\n\ndef propagate(x, edge_index, edge_weight=None):\n    edge_index, _ = add_remaining_self_loops(edge_index, num_nodes=x.size(0))\n\n    # calculate the degree normalize term\n    row, col = edge_index\n    deg = degree(col, x.size(0), dtype=x.dtype)\n    deg_inv_sqrt = deg.pow(-0.5)\n    # for the first order appro of laplacian matrix in GCN, we use deg_inv_sqrt[row]*deg_inv_sqrt[col]\n    if(edge_weight == None):\n        edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n\n    # normalize the features on the starting point of the edge\n    out = edge_weight.view(-1, 1) * x[row]\n\n    return scatter(out, edge_index[-1], dim=0, dim_size=x.size(0), reduce='add')\n\ndef propagate2(x, edge_index):\n    edge_index, _ = add_remaining_self_loops(\n        edge_index, num_nodes=x.size(0))\n\n    # calculate the degree normalize term\n    row, col = edge_index\n    deg = degree(col, x.size(0), dtype=x.dtype)\n    deg_inv_sqrt = deg.pow(-0.5)\n    # for the first order appro of laplacian matrix in GCN, we use deg_inv_sqrt[row]*deg_inv_sqrt[col]\n    edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n\n    # normalize the features on the starting point of the edge\n    out = edge_weight.view(-1, 1) * x[row]\n\n    return scatter(out, edge_index[-1], dim=0, dim_size=x.size(0), reduce='add')\n\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.allow_tf32 = False\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = True\n    # torch.use_deterministic_algorithms(True)\n\n\ndef fair_metric(pred, labels, sens):\n    idx_s0 = sens == 0\n    idx_s1 = sens == 1\n    idx_s0_y1 = np.bitwise_and(idx_s0, labels == 1)\n    idx_s1_y1 = np.bitwise_and(idx_s1, labels == 1)\n    parity = abs(sum(pred[idx_s0]) / sum(idx_s0) -\n                 sum(pred[idx_s1]) / sum(idx_s1))\n    equality = abs(sum(pred[idx_s0_y1]) / sum(idx_s0_y1) -\n                   sum(pred[idx_s1_y1]) / sum(idx_s1_y1))\n    return parity.item(), equality.item()\n\ndef random_drop_edges(adj, drop_prob):\n    mask = torch.rand(adj.size()) > drop_prob\n    adj = adj * mask\n    adj = adj + adj.t() - adj * adj.t()\n    return adj\n\n\n# def D(x1, x2):  # negative cosine similarity\n#     return -F.cosine_similarity(x1, x2, dim=-1).mean()\n\ndef D(x1, x2):\n    x1 = F.normalize(x1, dim=-1)\n    x2 = F.normalize(x2, dim=-1)\n    return 1 - F.cosine_similarity(x1, x2, dim=-1).mean()\n\n\ndef ContrastiveLoss(p, z, T=.07):\n    # normalize\n    p = F.normalize(p, dim=1)\n    z = F.normalize(z, dim=1)\n    # Einstein sum is more intuitive\n    logits = torch.einsum('nc,mc->nm', [p, z]) / T\n    labels = torch.arange(logits.shape[0], dtype=torch.long, device=logits.device)\n    return nn.CrossEntropyLoss()(logits, labels)\n\ndef CE(p, z):\n        return - (z.softmax(dim=1) * p.log_softmax(dim=1)).mean()\n","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:12.815841Z","iopub.execute_input":"2026-01-05T08:43:12.816095Z","iopub.status.idle":"2026-01-05T08:43:12.829867Z","shell.execute_reply.started":"2026-01-05T08:43:12.816062Z","shell.execute_reply":"2026-01-05T08:43:12.829077Z"},"papermill":{"duration":14.596635,"end_time":"2025-07-21T09:09:57.201180","exception":false,"start_time":"2025-07-21T09:09:42.604545","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"c91483f7","cell_type":"markdown","source":"# dataset","metadata":{"papermill":{"duration":0.006073,"end_time":"2025-07-21T09:09:57.213609","exception":false,"start_time":"2025-07-21T09:09:57.207536","status":"completed"},"tags":[]}},{"id":"3aa5a24a","cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport random\nfrom torch_geometric.utils import from_scipy_sparse_matrix\nimport scipy.sparse as sp\nfrom scipy.spatial import distance_matrix\nfrom torch_geometric.data import Data\nimport torch\nimport scipy.sparse as sp\n\n\ndef index_to_mask(node_num, index):\n    mask = torch.zeros(node_num, dtype=torch.bool)\n    mask[index] = 1\n\n    return mask\n\n\ndef sys_normalized_adjacency(adj):\n    adj = sp.coo_matrix(adj)\n    adj = adj + sp.eye(adj.shape[0])\n    row_sum = np.array(adj.sum(1))\n    row_sum = (row_sum == 0) * 1 + row_sum\n    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n\n    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\ndef feature_norm(features):\n    min_values = features.min(axis=0)[0]\n    max_values = features.max(axis=0)[0]\n    return 2 * (features - min_values).div(max_values - min_values) - 1\n\n\ndef build_relationship(x, thresh=0.25):\n    df_euclid = pd.DataFrame(\n        1 / (1 + distance_matrix(x.T.T, x.T.T)), columns=x.T.columns, index=x.T.columns)\n    df_euclid = df_euclid.to_numpy()\n    idx_map = []\n    for ind in range(df_euclid.shape[0]):\n        max_sim = np.sort(df_euclid[ind, :])[-2]\n        neig_id = np.where(df_euclid[ind, :] > thresh * max_sim)[0]\n        import random\n        random.seed(912)\n        random.shuffle(neig_id)\n        for neig in neig_id:\n            if neig != ind:\n                idx_map.append([ind, neig])\n    # print('building edge relationship complete')\n    idx_map = np.array(idx_map)\n\n    return idx_map\n\n\ndef load_credit(dataset, sens_attr=\"Age\", predict_attr=\"NoDefaultNextMonth\", path=\"./credit/\", label_number=1000):\n    # print('Loading {} dataset from {}'.format(dataset, path))\n    idx_features_labels = pd.read_csv(\n        os.path.join(path, \"{}.csv\".format(dataset)))\n    header = list(idx_features_labels.columns)\n    header.remove(predict_attr)\n    header.remove('Single')\n\n    # sensitive feature removal\n    # header.remove('Age')\n\n#    # Normalize MaxBillAmountOverLast6Months\n#    idx_features_labels['MaxBillAmountOverLast6Months'] = (idx_features_labels['MaxBillAmountOverLast6Months']-idx_features_labels['MaxBillAmountOverLast6Months'].mean())/idx_features_labels['MaxBillAmountOverLast6Months'].std()\n#\n#    # Normalize MaxPaymentAmountOverLast6Months\n#    idx_features_labels['MaxPaymentAmountOverLast6Months'] = (idx_features_labels['MaxPaymentAmountOverLast6Months'] - idx_features_labels['MaxPaymentAmountOverLast6Months'].mean())/idx_features_labels['MaxPaymentAmountOverLast6Months'].std()\n#\n#    # Normalize MostRecentBillAmount\n#    idx_features_labels['MostRecentBillAmount'] = (idx_features_labels['MostRecentBillAmount']-idx_features_labels['MostRecentBillAmount'].mean())/idx_features_labels['MostRecentBillAmount'].std()\n#\n#    # Normalize MostRecentPaymentAmount\n#    idx_features_labels['MostRecentPaymentAmount'] = (idx_features_labels['MostRecentPaymentAmount']-idx_features_labels['MostRecentPaymentAmount'].mean())/idx_features_labels['MostRecentPaymentAmount'].std()\n#\n#    # Normalize TotalMonthsOverdue\n#    idx_features_labels['TotalMonthsOverdue'] = (idx_features_labels['TotalMonthsOverdue']-idx_features_labels['TotalMonthsOverdue'].mean())/idx_features_labels['TotalMonthsOverdue'].std()\n\n    # build relationship\n    if os.path.exists(f'{path}/{dataset}_edges.txt'):\n        edges_unordered = np.genfromtxt(\n            f'{path}/{dataset}_edges.txt').astype('int')\n    else:\n        edges_unordered = build_relationship(\n            idx_features_labels[header], thresh=0.7)\n        np.savetxt(f'{path}/{dataset}_edges.txt', edges_unordered)\n\n    relate_feature_names = [\n        'EducationLevel',                # 受教育程度\n        'Married',                       # 婚姻状态\n        'MaxBillAmountOverLast6Months',  # 最近6个月最大账单\n        'MaxPaymentAmountOverLast6Months', # 最近6个月最大支付\n        'TotalMonthsOverdue'             # 逾期总月份\n    ]\n    relate_features = sp.csr_matrix(idx_features_labels[relate_feature_names], dtype=np.float32)\n\n    \n    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    header.remove('Age')\n    print(\"Credit 数据集特征：\", header)\n    features_nosens = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    \n    # print(features)\n    labels = idx_features_labels[predict_attr].values\n\n    idx = np.arange(features.shape[0])\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=int).reshape(edges_unordered.shape)\n\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n    adj = adj + sp.eye(adj.shape[0])\n    adj_norm = sys_normalized_adjacency(adj)\n    adj_norm_sp = sparse_mx_to_torch_sparse_tensor(adj_norm)\n\n    edge_index, _ = from_scipy_sparse_matrix(adj)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    relate_features = torch.FloatTensor(np.array(relate_features.todense()))\n    features_nosens = torch.FloatTensor(np.array(features_nosens.todense()))\n    labels = torch.LongTensor(labels)\n\n    #\n    #\n\n    import random\n    random.seed(20)\n    label_idx_0 = np.where(labels == 0)[0]\n    label_idx_1 = np.where(labels == 1)[0]\n    random.shuffle(label_idx_0)\n    random.shuffle(label_idx_1)\n\n    idx_train = np.append(label_idx_0[:min(int(0.5 * len(label_idx_0)), label_number // 2)],\n                          label_idx_1[:min(int(0.5 * len(label_idx_1)), label_number // 2)])\n    idx_val = np.append(label_idx_0[int(0.5 * len(label_idx_0)):int(0.75 * len(\n        label_idx_0))], label_idx_1[int(0.5 * len(label_idx_1)):int(0.75 * len(label_idx_1))])\n    idx_test = np.append(label_idx_0[int(\n        0.75 * len(label_idx_0)):], label_idx_1[int(0.75 * len(label_idx_1)):])\n\n    sens = idx_features_labels[sens_attr].values.astype(int)\n    sens = torch.LongTensor(sens)\n\n    train_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_train))\n    val_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_val))\n    test_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_test))\n    from collections import Counter\n    print('predict_attr:',Counter(idx_features_labels[predict_attr]))\n    print('sens_attr:',Counter(idx_features_labels[sens_attr]))\n    return adj_norm_sp, edge_index, features, features_nosens, relate_features, labels, train_mask, val_mask, test_mask, sens, adj\n\n\ndef load_bail(dataset, sens_attr=\"WHITE\", predict_attr=\"RECID\", path=\"./bail/\", label_number=1000):\n    # print('Loading {} dataset from {}'.format(dataset, path))\n    idx_features_labels = pd.read_csv(\n        os.path.join(path, \"{}.csv\".format(dataset)))\n    header = list(idx_features_labels.columns)\n    header.remove(predict_attr)\n\n    # sensitive feature removal\n    # header.remove('WHITE')\n\n    # # Normalize School\n    # idx_features_labels['SCHOOL'] = 2*(idx_features_labels['SCHOOL']-idx_features_labels['SCHOOL'].min()).div(idx_features_labels['SCHOOL'].max() - idx_features_labels['SCHOOL'].min()) - 1\n\n    # # Normalize RULE\n    # idx_features_labels['RULE'] = 2*(idx_features_labels['RULE']-idx_features_labels['RULE'].min()).div(idx_features_labels['RULE'].max() - idx_features_labels['RULE'].min()) - 1\n\n    # # Normalize AGE\n    # idx_features_labels['AGE'] = 2*(idx_features_labels['AGE']-idx_features_labels['AGE'].min()).div(idx_features_labels['AGE'].max() - idx_features_labels['AGE'].min()) - 1\n\n    # # Normalize TSERVD\n    # idx_features_labels['TSERVD'] = 2*(idx_features_labels['TSERVD']-idx_features_labels['TSERVD'].min()).div(idx_features_labels['TSERVD'].max() - idx_features_labels['TSERVD'].min()) - 1\n\n    # # Normalize FOLLOW\n    # idx_features_labels['FOLLOW'] = 2*(idx_features_labels['FOLLOW']-idx_features_labels['FOLLOW'].min()).div(idx_features_labels['FOLLOW'].max() - idx_features_labels['FOLLOW'].min()) - 1\n\n    # # Normalize TIME\n    # idx_features_labels['TIME'] = 2*(idx_features_labels['TIME']-idx_features_labels['TIME'].min()).div(idx_features_labels['TIME'].max() - idx_features_labels['TIME'].min()) - 1\n\n    # build relationship\n    if os.path.exists(f'{path}/{dataset}_edges.txt'):\n        edges_unordered = np.genfromtxt(\n            f'{path}/{dataset}_edges.txt').astype('int')\n    else:\n        edges_unordered = build_relationship(\n            idx_features_labels[header], thresh=0.6)\n        np.savetxt(f'{path}/{dataset}_edges.txt', edges_unordered)\n\n    relate_feature_names = [\n        'SCHOOL',   # 教育水平或上学经历\n        'WORKREL',  # 工作/就业关系\n        'PROPTY',   # 财产状况\n        'PRIORS',   # 过去犯罪记录数量\n        'FELON'    # 是否有重罪记录\n    ]\n    relate_features = sp.csr_matrix(idx_features_labels[relate_feature_names], dtype=np.float32)\n\n    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    header.remove('WHITE')\n    print(\"Bail 数据集特征：\", header)\n    features_nosens = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    \n    labels = idx_features_labels[predict_attr].values\n\n    idx = np.arange(features.shape[0])\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=int).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n    adj = adj + sp.eye(adj.shape[0])\n    adj_norm = sys_normalized_adjacency(adj)\n    adj_norm_sp = sparse_mx_to_torch_sparse_tensor(adj_norm)\n\n    edge_index, _ = from_scipy_sparse_matrix(adj)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    relate_features = torch.FloatTensor(np.array(relate_features.todense()))\n    features_nosens = torch.FloatTensor(np.array(features_nosens.todense()))\n    labels = torch.LongTensor(labels)\n\n    # print(features)\n\n    # features = normalize(features)\n    # adj = adj + sp.eye(adj.shape[0])\n\n    # features = torch.FloatTensor(np.array(features.todense()))\n    # labels = torch.LongTensor(labels)\n\n    import random\n    random.seed(20)\n    label_idx_0 = np.where(labels == 0)[0]\n    label_idx_1 = np.where(labels == 1)[0]\n    random.shuffle(label_idx_0)\n    random.shuffle(label_idx_1)\n    idx_train = np.append(label_idx_0[:min(int(0.5 * len(label_idx_0)), label_number // 2)],\n                          label_idx_1[:min(int(0.5 * len(label_idx_1)), label_number // 2)])\n    idx_val = np.append(label_idx_0[int(0.5 * len(label_idx_0)):int(0.75 * len(\n        label_idx_0))], label_idx_1[int(0.5 * len(label_idx_1)):int(0.75 * len(label_idx_1))])\n    idx_test = np.append(label_idx_0[int(\n        0.75 * len(label_idx_0)):], label_idx_1[int(0.75 * len(label_idx_1)):])\n\n    sens = idx_features_labels[sens_attr].values.astype(int)\n    sens = torch.LongTensor(sens)\n    train_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_train))\n    val_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_val))\n    test_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_test))\n    from collections import Counter\n    print('predict_attr:',Counter(idx_features_labels[predict_attr]))\n    print('sens_attr:',Counter(idx_features_labels[sens_attr]))\n    return adj_norm_sp, edge_index, features, features_nosens, relate_features, labels, train_mask, val_mask, test_mask, sens, adj\n\n\ndef load_german(dataset, sens_attr=\"Gender\", predict_attr=\"GoodCustomer\", path=\"./german/\", label_number=1000):\n    # print('Loading {} dataset from {}'.format(dataset, path))\n    idx_features_labels = pd.read_csv(\n        os.path.join(path, \"{}.csv\".format(dataset)))\n    header = list(idx_features_labels.columns)\n    header.remove(predict_attr)\n    header.remove('OtherLoansAtStore')\n    header.remove('PurposeOfLoan')\n\n    # Sensitive Attribute\n    idx_features_labels['Gender'][idx_features_labels['Gender']\n                                  == 'Female'] = 1\n    idx_features_labels['Gender'][idx_features_labels['Gender'] == 'Male'] = 0\n\n#    for i in range(idx_features_labels['PurposeOfLoan'].unique().shape[0]):\n#        val = idx_features_labels['PurposeOfLoan'].unique()[i]\n#        idx_features_labels['PurposeOfLoan'][idx_features_labels['PurposeOfLoan'] == val] = i\n\n#    # Normalize LoanAmount\n#    idx_features_labels['LoanAmount'] = 2*(idx_features_labels['LoanAmount']-idx_features_labels['LoanAmount'].min()).div(idx_features_labels['LoanAmount'].max() - idx_features_labels['LoanAmount'].min()) - 1\n#\n#    # Normalize Age\n#    idx_features_labels['Age'] = 2*(idx_features_labels['Age']-idx_features_labels['Age'].min()).div(idx_features_labels['Age'].max() - idx_features_labels['Age'].min()) - 1\n#\n#    # Normalize LoanDuration\n#    idx_features_labels['LoanDuration'] = 2*(idx_features_labels['LoanDuration']-idx_features_labels['LoanDuration'].min()).div(idx_features_labels['LoanDuration'].max() - idx_features_labels['LoanDuration'].min()) - 1\n#\n    # build relationship\n    if os.path.exists(f'{path}/{dataset}_edges.txt'):\n        edges_unordered = np.genfromtxt(\n            f'{path}/{dataset}_edges.txt').astype('int')\n    else:\n        edges_unordered = build_relationship(\n            idx_features_labels[header], thresh=0.8)\n        np.savetxt(f'{path}/{dataset}_edges.txt', edges_unordered)\n\n    relate_features_names = [\n        'Single',               # 婚姻状态\n        'HasCoapplicant',       # 是否有共同贷款申请人\n        'YearsAtCurrentJob_lt_1',  # 当前工作年限<1\n        'YearsAtCurrentJob_geq_4', # 当前工作年限>=4\n        'JobClassIsSkilled',    # 是否属于熟练职业\n        'Age' ]                 # 年龄\n \n    relate_features = sp.csr_matrix(idx_features_labels[relate_features_names], dtype=np.float32)\n    \n    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    \n    header.remove('Gender')\n    print(\"German 数据集特征：\", header)\n    features_nosens = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    \n    labels = idx_features_labels[predict_attr].values\n    labels[labels == -1] = 0\n\n    idx = np.arange(features.shape[0])\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=int).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n    adj = adj + sp.eye(adj.shape[0])\n\n    adj_norm = sys_normalized_adjacency(adj)\n    adj_norm_sp = sparse_mx_to_torch_sparse_tensor(adj_norm)\n\n    edge_index, _ = from_scipy_sparse_matrix(adj)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    relate_features = torch.FloatTensor(np.array(relate_features.todense()))\n    features_nosens = torch.FloatTensor(np.array(features_nosens.todense()))\n    \n    labels = torch.LongTensor(labels)\n\n    # features = torch.FloatTensor(np.array(features.todense()))\n    # labels = torch.LongTensor(labels)\n\n    import random\n    random.seed(20)\n    label_idx_0 = np.where(labels == 0)[0]\n    label_idx_1 = np.where(labels == 1)[0]\n    random.shuffle(label_idx_0)\n    random.shuffle(label_idx_1)\n    idx_train = np.append(label_idx_0[:min(int(0.5 * len(label_idx_0)), label_number // 2)],\n                          label_idx_1[:min(int(0.5 * len(label_idx_1)), label_number // 2)])\n    idx_val = np.append(label_idx_0[int(0.5 * len(label_idx_0)):int(0.75 * len(\n        label_idx_0))], label_idx_1[int(0.5 * len(label_idx_1)):int(0.75 * len(label_idx_1))])\n    idx_test = np.append(label_idx_0[int(\n        0.75 * len(label_idx_0)):], label_idx_1[int(0.75 * len(label_idx_1)):])\n\n    sens = idx_features_labels[sens_attr].values.astype(int)\n    sens = torch.LongTensor(sens)\n\n    train_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_train))\n    val_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_val))\n    test_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_test))\n    from collections import Counter\n    print('predict_attr:',Counter(idx_features_labels[predict_attr]))\n    print('sens_attr:',Counter(idx_features_labels[sens_attr]))\n    return adj_norm_sp, edge_index, features, features_nosens, relate_features, labels, train_mask, val_mask, test_mask, sens, adj\n\ndef load_pokec(dataset,sens_attr=\"region\",predict_attr=\"I_am_working_in_field\", path=\"./pokec/\", label_number=3000,sens_number=500,seed=20,test_idx=True):\n    \"\"\"Load data\"\"\"\n    print('Loading {} dataset from {}'.format(dataset,path))\n\n    idx_features_labels = pd.read_csv(os.path.join(path,\"{}.csv\".format(dataset)))\n    header = list(idx_features_labels.columns)\n    header.remove(\"user_id\")\n\n    # header.remove(sens_attr)\n    header.remove(predict_attr)\n\n    if(dataset == 'region_job_2'):\n        relate_feature_names = [\n             # 语言\n            'spoken_languages_indicator', 'anglicky', 'nemecky', 'rusky', 'francuzsky', 'taliansky', 'slovensky', 'japonsky',\n             # 兴趣爱好\n            'hobbies_indicator', 'sportovanie', 'pozeranie filmov', 'surfovanie po webe', 'turistika',\n             # 饮食偏好\n            'I_most_enjoy_good_food_indicator', 'I_like_specialties_from_kitchen_indicator',\n             # 音乐偏好\n            'I_mostly_like_listening_to_music_indicator']\n    elif(dataset == 'region_job'):\n        print(\"进来了\")\n        relate_feature_names = [\n            'spoken_languages_indicator', 'anglicky', 'nemecky', 'rusky', 'francuzsky', 'talianskej', 'slovenskej', 'japonskej',\n            'hobbies_indicator', 'sportovanie', 'pozeranie filmov', 'surfovanie po webe', 'turistika',\n            'I_most_enjoy_good_food_indicator', 'I_like_specialties_from_kitchen_indicator',\n            'I_mostly_like_listening_to_music_indicator']\n\n    \n    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    relate_features = sp.csr_matrix(idx_features_labels[relate_feature_names], dtype=np.float32)\n    header.remove('region')\n    print(dataset,\"数据集特征：\", header)\n    features_nosens = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n    labels = idx_features_labels[predict_attr].values\n    \n    \n    # build graph\n    idx = np.array(idx_features_labels[\"user_id\"], dtype=int)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(os.path.join(path,\"{}_relationship.txt\".format(dataset)), dtype=int)\n\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=int).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    # features = normalize(features)\n    adj = adj + sp.eye(adj.shape[0])\n\n    adj_norm = sys_normalized_adjacency(adj)\n    adj_norm_sp = sparse_mx_to_torch_sparse_tensor(adj_norm)\n\n    edge_index, _ = from_scipy_sparse_matrix(adj)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    relate_features = torch.FloatTensor(np.array(relate_features.todense()))\n    features_nosens = torch.FloatTensor(np.array(features_nosens.todense()))\n    # num_classes = len(idx_features_labels[predict_attr].unique()) - 1\n    # labels = torch.eye(num_classes)[labels]\n    labels = torch.LongTensor(labels)\n    # adj = sparse_mx_to_torch_sparse_tensor(adj)\n\n    # import random\n    # random.seed(seed)\n    # label_idx = np.where(labels>=0)[0]\n    # random.shuffle(label_idx)\n\n    # idx_train = label_idx[:min(int(0.5 * len(label_idx)),label_number)]\n    # idx_val = label_idx[int(0.5 * len(label_idx)):int(0.75 * len(label_idx))]\n    # if test_idx:\n    #     idx_test = label_idx[label_number:]\n    #     idx_val = idx_test\n    # else:\n    #     idx_test = label_idx[int(0.75 * len(label_idx)):]\n\n    import random\n    random.seed(20)\n    label_idx_0 = np.where(labels == 0)[0]\n    label_idx_1 = np.where(labels > 0)[0]\n    random.shuffle(label_idx_0)\n    random.shuffle(label_idx_1)\n    idx_train = np.append(label_idx_0[:min(int(0.5 * len(label_idx_0)), label_number // 2)],\n                          label_idx_1[:min(int(0.5 * len(label_idx_1)), label_number // 2)])\n    idx_val = np.append(label_idx_0[int(0.5 * len(label_idx_0)):int(0.75 * len(\n        label_idx_0))], label_idx_1[int(0.5 * len(label_idx_1)):int(0.75 * len(label_idx_1))])\n    idx_test = np.append(label_idx_0[int(\n        0.75 * len(label_idx_0)):], label_idx_1[int(0.75 * len(label_idx_1)):])\n\n\n    sens = idx_features_labels[sens_attr].values\n\n    sens_idx = set(np.where(sens >= 0)[0])\n    idx_test = np.asarray(list(sens_idx & set(idx_test)))\n    sens = torch.FloatTensor(sens)\n    idx_sens_train = list(sens_idx - set(idx_val) - set(idx_test))\n\n    random.shuffle(idx_sens_train)\n    idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n    train_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_train))\n    val_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_val))\n    test_mask = index_to_mask(features.shape[0], torch.LongTensor(idx_test))\n\n    # pokec data division\n    labels[labels>1]=1\n    if sens_attr:\n        sens[sens>0]=1\n        \n    from collections import Counter\n    print('predict_attr:',Counter(idx_features_labels[predict_attr]))\n    print('sens_attr:',Counter(idx_features_labels[sens_attr]))\n    print('total dimension:', features.shape)\n    # random.shuffle(sens_idx)\n\n    return adj_norm_sp, edge_index, features, features_nosens, relate_features, labels, train_mask, val_mask, test_mask, sens, adj \n\ndef get_dataset(dataname):\n    if(dataname == 'credit'):\n        load, label_num = load_credit, 6000\n    elif(dataname == 'bail'):\n        load, label_num = load_bail, 100\n    elif(dataname == 'german'):\n        load, label_num = load_german, 100\n    elif(dataname == 'pokec_z'):\n        dataname = 'region_job'\n        load, label_num = load_pokec, 3000\n    elif(dataname == 'pokec_n'):\n        dataname = 'region_job_2'\n        load, label_num = load_pokec, 3000\n\n    adj_norm_sp, edge_index, features, features_nosens , relate_features, labels, train_mask, val_mask, test_mask, sens, adj = load(\n        dataset=dataname, label_number=label_num)\n\n    if(dataname == 'credit'):\n        sens_idx = 1\n    elif(dataname == 'bail' or dataname == 'german'):\n        sens_idx = 0\n    elif(dataname == 'region_job' or dataname == 'region_job_2'):\n        sens_idx = 3\n\n    x_max, x_min = torch.max(features, dim=0)[\n        0], torch.min(features, dim=0)[0]\n    \n    if(dataname != 'german'):\n        norm_features = feature_norm(features)\n        norm_features[:, sens_idx] = features[:, sens_idx]\n        features = norm_features\n\n    #wwj\n    x_one = torch.ones_like(features)\n    x_no_s_one = features_nosens\n     \n    return Data(adj=adj, x=features, x_f = features, x_one=x_one, x_no_s_one = x_no_s_one, x_no_s = features_nosens, x_relate = relate_features, edge_index=edge_index, adj_norm_sp=adj_norm_sp, y=labels.float(), train_mask=train_mask, val_mask=val_mask, test_mask=test_mask, sens=sens ,pred_s = sens), sens_idx, x_min, x_max\n","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:12.831721Z","iopub.execute_input":"2026-01-05T08:43:12.832010Z","iopub.status.idle":"2026-01-05T08:43:12.888305Z","shell.execute_reply.started":"2026-01-05T08:43:12.831990Z","shell.execute_reply":"2026-01-05T08:43:12.887454Z"},"papermill":{"duration":0.066619,"end_time":"2025-07-21T09:09:57.286468","exception":false,"start_time":"2025-07-21T09:09:57.219849","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"id":"8c5e69d1-682b-4fa0-aec0-2042df34f467","cell_type":"markdown","source":"# Generate pseudo-sensitive attributes","metadata":{}},{"id":"92625373-65dc-41c6-8637-bf3b5750b3e5","cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\n\n# ===== VGAE 模型部分（示例） =====\nfrom torch_geometric.nn import VGAE, GCNConv\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\nclass GCNEncoder(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(GCNEncoder, self).__init__()\n        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n        self.conv_logvar = GCNConv(2 * out_channels, out_channels)\n\n    def forward(self, x, edge_index):\n        x = F.relu(self.conv1(x, edge_index))\n        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n\n\ndef get_pseudo_sens(data,args):\n    dataset_name = args.dataset  # 可换成 credit / bail / pokec_n / pokec_z\n    print(\"===\" + dataset_name + \" ===\")\n    #data, sens_idx, x_min, x_max = get_dataset(dataset_name)\n\n    if(args.input_x == 'all'):\n        in_dim = data.x_no_s.shape[1]\n        num_nodes = data.x_no_s.shape[0]\n    elif (args.input_x == 'relate'):\n        in_dim = data.x_relate.shape[1]\n        num_nodes = data.x_relate.shape[0]\n    \n    hidden_dim = 16\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    encoder = GCNEncoder(in_dim, hidden_dim)\n    model = VGAE(encoder).to(device)\n    data = data.to(device)\n    \n    # VGAE 训练\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    model.train()\n    for epoch in range(100):\n        optimizer.zero_grad()\n        if(args.input_x == 'all'):\n            z = model.encode(data.x_no_s, data.edge_index)\n        elif(args.input_x == 'relate'):\n            z = model.encode(data.x_relate, data.edge_index)\n        loss = model.recon_loss(z, data.edge_index)\n        loss = loss + (1 / num_nodes) * model.kl_loss()\n        loss.backward()\n        optimizer.step()\n        # if epoch % 20 == 0:\n        #     print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n    \n    # 得到节点嵌入\n    model.eval()\n    if(args.input_x == 'all'):\n        z = model.encode(data.x_no_s, data.edge_index).detach().cpu().numpy()\n    elif(args.input_x == 'relate'):\n        z = model.encode(data.x_relate, data.edge_index).detach().cpu().numpy()\n\n    \n    # ===== 可选增强：PCA 降维 =====\n    pca = PCA(n_components=4)  # 降到 4 维，也可以改成 2~4\n    z_reduced = pca.fit_transform(z)\n\n    # ====== 计算准确率 & 分布 ======\n    true_sens = data.sens.cpu().numpy()\n    \n\n    if(args.jl == 'GMM'):\n        # 保存到 data\n        # ====== GMM 聚类 ======\n        gmm = GaussianMixture(n_components=2, covariance_type=\"full\", random_state=42, n_init=10)\n        pred_s = gmm.fit_predict(z_reduced)\n\n        # ===== 可选增强：平滑扰动 =====\n        # 将 pred_s 转为 float，加一点随机噪声，再取 0/1\n        noise_strength = 0.05  # 噪声强度，可调\n        pred_s = pred_s.astype(float) + np.random.uniform(-noise_strength, noise_strength, size=pred_s.shape)\n        pred_s = (pred_s > 0.5).astype(int)  # 重新二值化\n\n        acc_gmm = max(accuracy_score(true_sens, pred_s),accuracy_score(true_sens, 1 - pred_s) )\n        \n        print(\"GMM 预测准确率:\", acc_gmm)\n        print(\"GMM 伪s分布:\", Counter(pred_s))\n        \n    else:\n        # ====== KMeans 聚类 ======\n        kmeans = KMeans(n_clusters=2, random_state=42, n_init=20)\n        pred_s = kmeans.fit_predict(z_reduced)\n\n        # ===== 可选增强：平滑扰动 =====\n        # 将 pred_s 转为 float，加一点随机噪声，再取 0/1\n        noise_strength = 0.05  # 噪声强度，可调\n        pred_s = pred_s.astype(float) + np.random.uniform(-noise_strength, noise_strength, size=pred_s.shape)\n        pred_s = (pred_s > 0.5).astype(int)  # 重新二值化\n        \n        # KMeans 可能会有 label 对调问题，所以取 max(准确率, 1-准确率)\n        acc_kmeans = max(\n            accuracy_score(true_sens, pred_s),\n            accuracy_score(true_sens, 1 - pred_s)\n        )\n        print(\"KMeans 预测准确率:\", acc_kmeans)\n        print(\"KMeans 伪s分布:\", Counter(pred_s))\n        \n    \n    pred_s = torch.tensor(pred_s).to(device)\n    return pred_s\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:43:12.889424Z","iopub.execute_input":"2026-01-05T08:43:12.889743Z","iopub.status.idle":"2026-01-05T08:43:12.908014Z","shell.execute_reply.started":"2026-01-05T08:43:12.889695Z","shell.execute_reply":"2026-01-05T08:43:12.907434Z"}},"outputs":[],"execution_count":21},{"id":"f812e063","cell_type":"markdown","source":"# model","metadata":{"papermill":{"duration":0.005913,"end_time":"2025-07-21T09:09:57.299622","exception":false,"start_time":"2025-07-21T09:09:57.293709","status":"completed"},"tags":[]}},{"id":"bfd01c76","cell_type":"code","source":"from torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Parameter\nfrom torch_geometric.nn import GINConv, SAGEConv\nfrom torch.nn.utils import spectral_norm\n\n\nclass MLP_projector(torch.nn.Module):\n    def __init__(self, args):\n        super(MLP_projector, self).__init__()\n        self.args = args\n\n        self.lin = Linear(args.hidden * 2, args.hidden)\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n\n    def forward(self, x, edge_index=None, mask_node=None):\n        h = self.lin(x)\n\n        return h\n\n\nclass MLP_discriminator(torch.nn.Module):\n    def __init__(self, args):\n        super(MLP_discriminator, self).__init__()\n        self.args = args\n\n        self.lin = Linear(args.hidden, 1)\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n\n    def forward(self, h, edge_index=None, mask_node=None):\n        h = self.lin(h)\n\n        return torch.sigmoid(h)\n\n\nclass MLP_encoder(torch.nn.Module):\n    def __init__(self, args):\n        super(MLP_encoder, self).__init__()\n        self.args = args\n\n        self.lin = Linear(args.num_features, args.hidden)\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n\n    def forward(self, x, edge_index=None, mask_node=None):\n        h = self.lin(x)\n\n        return h\n\n\nclass GCN_encoder_scatter(torch.nn.Module):\n    def __init__(self, args):\n        super(GCN_encoder_scatter, self).__init__()\n\n        self.args = args\n\n        self.lin = Linear(args.num_features, args.hidden, bias=False)\n\n        self.bias = Parameter(torch.Tensor(args.hidden))\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n        self.bias.data.fill_(0.0)\n\n    def forward(self, x, edge_index, adj_norm_sp):\n        h = self.lin(x)\n        h = propagate2(h, edge_index) + self.bias\n        return h\n\n\nclass GCN_encoder_spmm(torch.nn.Module):\n    def __init__(self, args):\n        super(GCN_encoder_spmm, self).__init__()\n\n        self.args = args\n\n        self.lin = Linear(args.num_features, args.hidden, bias=False)\n        self.bias = Parameter(torch.Tensor(args.hidden))\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n        self.bias.data.fill_(0.0)\n\n    def forward(self, x, edge_index, adj_norm_sp):\n        h = self.lin(x)\n        h = torch.spmm(adj_norm_sp, h) + self.bias\n\n        return h\n\n\nclass GIN_encoder(nn.Module):\n    def __init__(self, args):\n        super(GIN_encoder, self).__init__()\n\n        self.args = args\n\n        self.mlp = nn.Sequential(\n            nn.Linear(args.num_features, args.hidden),\n            # nn.ReLU(),\n            nn.BatchNorm1d(args.hidden),\n            # nn.Linear(args.hidden, args.hidden),\n        )\n\n        self.conv = GINConv(self.mlp)\n\n    def reset_parameters(self):\n        self.conv.reset_parameters()\n\n    def forward(self, x, edge_index, adj_norm_sp):\n        h = self.conv(x, edge_index)\n        return h\n\n\nclass SAGE_encoder(nn.Module):\n    def __init__(self, args):\n        super(SAGE_encoder, self).__init__()\n\n        self.args = args\n\n        self.conv1 = SAGEConv(args.num_features, args.hidden, normalize=True)\n        self.conv1.aggr = 'mean'\n        self.transition = nn.Sequential(\n            nn.ReLU(),\n            nn.BatchNorm1d(args.hidden),\n            nn.Dropout(p=args.dropout)\n        )\n        self.conv2 = SAGEConv(args.hidden, args.hidden, normalize=True)\n        self.conv2.aggr = 'mean'\n\n    def reset_parameters(self):\n        self.conv1.reset_parameters()\n        self.conv2.reset_parameters()\n\n    def forward(self, x, edge_index, adj_norm_sp):\n        x = self.conv1(x, edge_index)\n        x = self.transition(x)\n        h = x\n        # h = self.conv2(x, edge_index)\n        return h\n\n\nclass MLP_classifier(torch.nn.Module):\n    def __init__(self, args):\n        super(MLP_classifier, self).__init__()\n        self.args = args\n\n        self.lin = Linear(args.hidden, args.num_classes)\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n\n    def forward(self, h, edge_index=None):\n        h = self.lin(h)\n\n        return h\n","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:12.908824Z","iopub.execute_input":"2026-01-05T08:43:12.909034Z","iopub.status.idle":"2026-01-05T08:43:12.928920Z","shell.execute_reply.started":"2026-01-05T08:43:12.909015Z","shell.execute_reply":"2026-01-05T08:43:12.928348Z"},"papermill":{"duration":0.030562,"end_time":"2025-07-21T09:09:57.336336","exception":false,"start_time":"2025-07-21T09:09:57.305774","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"id":"40e0efcc","cell_type":"markdown","source":"# evaluation","metadata":{"papermill":{"duration":0.00621,"end_time":"2025-07-21T09:09:57.348743","exception":false,"start_time":"2025-07-21T09:09:57.342533","status":"completed"},"tags":[]}},{"id":"1a7cc919","cell_type":"code","source":"import torch.nn.functional as F\nimport torch\nfrom sklearn.metrics import f1_score, roc_auc_score\n\ndef evaluate(x, classifier, hp, encoder_m, encoder_g, projector, data, args):\n    classifier.eval()\n    encoder_m.eval()\n    encoder_g.eval()\n    projector.eval()\n\n    with torch.no_grad():\n        if(args.have_s == 'yes' or args.have_s == 'preds'):\n            h_m = encoder_m(data.x , data.edge_index, data.adj_norm_sp)\n            h_g = encoder_g(data.x_one, data.edge_index, data.adj_norm_sp)\n            #print(\"eval input features have sens\")\n        elif (args.have_s == 'nos'):\n            h_m = encoder_m(data.x_no_s , data.edge_index, data.adj_norm_sp)\n            h_g = encoder_g(data.x_no_s_one, data.edge_index, data.adj_norm_sp)\n            #print(\"eval input features don't have  sens \")\n        \n        h_c = torch.cat((h_m, h_g), dim=1)\n        h = projector(h_c)\n        output = classifier(h)\n\n    accs, auc_rocs, F1s, paritys, equalitys = {}, {}, {}, {}, {}\n\n    pred_val = (output[data.val_mask].squeeze() > 0).type_as(data.y)\n    pred_test = (output[data.test_mask].squeeze() > 0).type_as(data.y)\n\n    accs['val'] = pred_val.eq(\n        data.y[data.val_mask]).sum().item() / data.val_mask.sum().item()\n    accs['test'] = pred_test.eq(\n        data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n\n    F1s['val'] = f1_score(data.y[data.val_mask].cpu(\n    ).numpy(), pred_val.cpu().numpy())\n\n    F1s['test'] = f1_score(data.y[data.test_mask].cpu(\n    ).numpy(), pred_test.cpu().numpy())\n\n    auc_rocs['val'] = roc_auc_score(\n        data.y[data.val_mask].cpu().numpy(), output[data.val_mask].detach().cpu().numpy())\n    auc_rocs['test'] = roc_auc_score(\n        data.y[data.test_mask].cpu().numpy(), output[data.test_mask].detach().cpu().numpy())\n    \n    \n    paritys['val'], equalitys['val'] = fair_metric(pred_val.cpu().numpy(), data.y[data.val_mask].cpu(\n    ).numpy(), data.pred_s[data.val_mask].cpu().numpy())\n    paritys['test'], equalitys['test'] = fair_metric(pred_test.cpu().numpy(), data.y[data.test_mask].cpu(\n    ).numpy(), data.sens[data.test_mask].cpu().numpy())\n\n    return accs, auc_rocs, F1s, paritys, equalitys\n","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:12.929812Z","iopub.execute_input":"2026-01-05T08:43:12.930058Z","iopub.status.idle":"2026-01-05T08:43:12.943667Z","shell.execute_reply.started":"2026-01-05T08:43:12.930033Z","shell.execute_reply":"2026-01-05T08:43:12.943153Z"},"papermill":{"duration":1.122623,"end_time":"2025-07-21T09:09:58.477486","exception":false,"start_time":"2025-07-21T09:09:57.354863","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":23},{"id":"8a41eb54","cell_type":"markdown","source":"# DFN","metadata":{"papermill":{"duration":0.005911,"end_time":"2025-07-21T09:09:58.489748","exception":false,"start_time":"2025-07-21T09:09:58.483837","status":"completed"},"tags":[]}},{"id":"568b41ca","cell_type":"code","source":"import argparse\nfrom tqdm import tqdm\nfrom torch import tensor\nimport warnings\n\nwarnings.filterwarnings('ignore')\nimport math\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport time\nfrom memory_profiler import memory_usage\n\n\nclass MLP(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def reset_parameters(self):\n        self.fc1.reset_parameters()\n        self.fc2.reset_parameters()\n        self.fc3.reset_parameters()\n\n\ndef run(data, args):\n    pbar = tqdm(range(args.runs), unit='run')\n    criterion = nn.BCELoss()\n    acc, f1, auc_roc, parity, equality = np.zeros(args.runs), np.zeros(\n        args.runs), np.zeros(args.runs), np.zeros(args.runs), np.zeros(args.runs)\n\n    data = data.to(args.device)\n\n    # projector\n    projector = MLP_projector(args).to(args.device)\n    optimizer_p = torch.optim.Adam([\n        dict(params=projector.lin.parameters(), weight_decay=args.e_wd)], lr=args.e_lr)\n\n    #discriminator\n    discriminator = MLP_discriminator(args).to(args.device)\n    optimizer_d = torch.optim.Adam([\n        dict(params=discriminator.lin.parameters(), weight_decay=args.d_wd)], lr=args.d_lr)\n\n    # classifier\n    classifier = MLP_classifier(args).to(args.device)\n    optimizer_c = torch.optim.Adam([\n        dict(params=classifier.lin.parameters(), weight_decay=args.c_wd)], lr=args.c_lr)\n\n    # original feature + MLP\n    encoder_m = MLP_encoder(args).to(args.device)\n    optimizer_em = torch.optim.Adam([\n        dict(params=encoder_m.lin.parameters(), weight_decay=args.e_wd)], lr=args.e_lr)\n\n    # # original feature + MLP\n    # encoder_mf = MLP_encoder(args).to(args.device)\n    # optimizer_emf = torch.optim.Adam([\n    #     dict(params=encoder_m.lin.parameters(), weight_decay=args.e_wd)], lr=args.e_lr)\n    \n    # all-one feature + MLP\n    if args.prop == 'scatter':\n        encoder_g = GCN_encoder_scatter(args).to(args.device)\n    else:\n        encoder_g = GCN_encoder_spmm(args).to(args.device)\n    optimizer_eg = torch.optim.Adam([\n        dict(params=encoder_g.lin.parameters(), weight_decay=args.e_wd),\n        dict(params=encoder_g.bias, weight_decay=args.e_wd)], lr=args.e_lr)\n\n\n\n    for count in pbar:\n        seed_everything(count + args.seed)\n        discriminator.reset_parameters()\n        classifier.reset_parameters()\n        encoder_m.reset_parameters()\n        #encoder_mf.reset_parameters()\n        encoder_g.reset_parameters()\n        projector.reset_parameters()\n\n        best_val_tradeoff = 0\n        best_val_loss = math.inf\n\n        for epoch in range(0, args.epochs):\n            # train classifier\n            classifier.train()\n            encoder_m.train()\n            #encoder_mf.train()\n            encoder_g.train()\n            projector.train()\n            for epoch_c in range(0, args.c_epochs):\n                optimizer_c.zero_grad()\n                optimizer_em.zero_grad()\n                #optimizer_emf.zero_grad()\n                optimizer_eg.zero_grad()\n\n                if(args.have_s == 'yes' or args.have_s == 'preds'):\n                    h_m = encoder_m(data.x , data.edge_index, data.adj_norm_sp)\n                    h_mf = encoder_m(data.x_f,data.edge_index, data.adj_norm_sp)\n                    h_g = encoder_g(data.x_one , data.edge_index, data.adj_norm_sp)\n                    #print(\"train input features have sens\")\n                elif args.have_s == 'nos' :\n                    h_m = encoder_m(data.x_no_s , data.edge_index, data.adj_norm_sp)\n                    h_g = encoder_g(data.x_no_s_one , data.edge_index, data.adj_norm_sp)\n                    #print(\"train input features don't have  sens \")\n\n                # l1 = D(h_m[data.train_mask], h_mf[data.train_mask]) / 2\n                # l2 = D(h_mf[data.train_mask], h_m[data.train_mask]) / 2\n                # l1 = ContrastiveLoss(h_m[data.train_mask], h_mf[data.train_mask]) / 2\n                # l2 = ContrastiveLoss(h_mf[data.train_mask], h_m[data.train_mask]) / 2\n                # l1 = CE(h_m[data.train_mask], h_mf[data.train_mask]) / 2\n                # l2 = CE(h_mf[data.train_mask], h_m[data.train_mask]) / 2\n                # align_loss_h = args.lambda_align * (l1 + l2)\n                \n                h_c = torch.cat((h_m, h_g), dim=1)\n                h_cf = torch.cat((h_mf, h_g), dim=1)\n                h = projector(h_c)\n                hf = projector(h_cf)\n\n                # 表征对齐\n                align_loss_h = D(h[data.train_mask], hf[data.train_mask])\n                \n                output = classifier(h)\n                outputf = classifier(hf)\n\n                # 更合理的替代版本 预测对齐\n                p = torch.sigmoid(output)\n                q = torch.sigmoid(outputf)\n                align_loss_y = 0.5 * (\n                    F.binary_cross_entropy(q, p.detach()) +\n                    F.binary_cross_entropy(p, q.detach())\n                )\n\n                \n                #align_loss_y = F.mse_loss(output[data.train_mask], outputf[data.train_mask])\n                #align_loss_y = F.binary_cross_entropy_with_logits(output[data.train_mask], outputf[data.train_mask])\n                loss_c1 = F.binary_cross_entropy_with_logits(output[data.train_mask], data.y[data.train_mask].unsqueeze(1).to(args.device))/2\n                loss_c2 = F.binary_cross_entropy_with_logits(outputf[data.train_mask], data.y[data.train_mask].unsqueeze(1).to(args.device))/2\n\n                # 总损失\n                if(args.flip == 'no'):\n                    loss_c = loss_c1\n                else:\n                    #loss_c = loss_c1 + align_loss_h\n                    #loss_c = loss_c1\n                    #loss_c = loss_c1 + loss_c2 + args.lambda_align*align_loss_y \n                    loss_c = loss_c1 + loss_c2 + args.lambda_h * align_loss_h + args.lambda_y * align_loss_y \n\n                loss_c.backward()\n\n                optimizer_em.step()\n                optimizer_eg.step()\n                optimizer_c.step()\n                optimizer_p.step()\n\n            \n\n            # evaluate classifier\n            accs, auc_rocs, F1s, tmp_parity, tmp_equality = evaluate(\n                data.x, classifier, discriminator, encoder_m, encoder_g, projector, data, args)\n\n            # print(epoch, 'Acc:', accs['test'], 'F1:', F1s['test'],\n            #       'Parity:', tmp_parity['test'], 'Equality:', tmp_equality['test'], 'tradeoff:',\n            #       auc_rocs['test'] + F1s['test'] + accs['test'] - args.alpha * (\n            #                   tmp_parity['test'] + tmp_equality['test']))\n            # if auc_rocs['val'] + F1s['val'] + accs['val'] - args.alpha * (\n            #         tmp_parity['val'] + tmp_equality['val']) > best_val_tradeoff:\n\n            # 验证集的敏感属性用的是预测的sens\n            if auc_rocs['val'] + F1s['val'] + accs['val']  - args.alpha * (\n                     tmp_parity['val'] + tmp_equality['val']) > best_val_tradeoff:\n                test_acc = accs['test']\n                test_auc_roc = auc_rocs['test']\n                test_f1 = F1s['test']\n                test_parity, test_equality = tmp_parity['test'], tmp_equality['test']\n                # print('best_val_tradeoff', epoch)\n                best_val_tradeoff = auc_rocs['val'] + F1s['val'] + accs['val'] - args.alpha * (\n                            tmp_parity['val'] + tmp_equality['val'])\n\n        acc[count] = test_acc\n        f1[count] = test_f1\n        auc_roc[count] = test_auc_roc\n        parity[count] = test_parity\n        equality[count] = test_equality\n\n    return acc, f1, auc_roc, parity, equality\n\n\n","metadata":{"execution":{"iopub.status.busy":"2026-01-05T08:43:12.944668Z","iopub.execute_input":"2026-01-05T08:43:12.945213Z","iopub.status.idle":"2026-01-05T08:43:12.964333Z","shell.execute_reply.started":"2026-01-05T08:43:12.945183Z","shell.execute_reply":"2026-01-05T08:43:12.963568Z"},"papermill":{"duration":0.039209,"end_time":"2025-07-21T09:09:58.534992","exception":false,"start_time":"2025-07-21T09:09:58.495783","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":24},{"id":"7eea6c83-37d3-4eb8-a3ec-1c7dd950f8d6","cell_type":"markdown","source":"# credit","metadata":{}},{"id":"5e589252-2f32-4641-b8da-bf0a6ee77cc6","cell_type":"code","source":"# 替换 argparse 的部分\nimport torch\nimport numpy as np\nfrom itertools import product\n\n# 模拟 argparse.Namespace，手动赋值\nclass Args:\n    dataset = 'credit'\n    runs = 5\n    epochs = 10\n    d_epochs = 5\n    c_epochs = 10\n    d_lr = 0.01\n    d_wd = 0\n    c_lr = 0.1\n    c_wd = 0.001\n    e_lr = 0.1\n    e_wd = 0.001\n    prop = 'scatter'\n    dropout = 0.5\n    hidden = 18\n    seed = 0\n    alpha = 1\n    delta = 0.5\n    m_epoch = 20\n    d = 'no'\n    have_s = 'preds' # preds , yes-with s\n    jl = 'Kmeans' #  Kmeans\n    input_x = 'relate' \n    lambda_h = 4 \n    lambda_y = 1\n    m_lr = 0.1\n    flip = 'yes' \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nargs = Args()\n\n# 获取数据\ndata, args.sens_idx, args.x_min, args.x_max = get_dataset(args.dataset)\n\n# # 参数搜索范围\n# lambda_y_list = [0.1, 0.2, 0.4, 0.6, 0.8, 1]\n# lambda_h_list = [0.01, 0.02, 0.04, 0.06, 0.08,0.1]\n# alpha_list = [0, 0.2, 0.4, 0.6, 0.8, 1 ]\n# for lambda_y_val,lambda_h_val in product(lambda_y_list,lambda_h_list):\n    \n# 设置当前的lambda_align值\n# args.lambda_y = lambda_y_val\n# args.lambda_h = lambda_h_val\n\nif args.have_s == 'nos':\n    args.num_features, args.num_classes = data.x_no_s.shape[1], 2 - 1  # binary classes are 0,1\n    print(\"numfeatures:\",args.num_features)\nelif args.have_s == 'yes':\n    args.num_features, args.num_classes = data.x.shape[1], 2 - 1  # binary classes are 0,1\nelse :  # hava_s == 'preds'\n    \n    data.pred_s = get_pseudo_sens(data, args)\n    \n    # 替换敏感属性列\n    modified_x = data.x.clone()\n    modified_x[:, args.sens_idx] = data.pred_s  # 确保类型匹配\n    # 更新data对象中的x\n    data.x = modified_x\n    \n    args.num_features, args.num_classes = data.x.shape[1], 2 - 1  # binary classes are 0,1\n\n# 运行主程序\nacc, f1, auc_roc, parity, equality = run(data, args)\n\n# 输出结果\nprint('======' + args.dataset + '======')\nprint(f\"lambda_y={args.lambda_y},lambda_y={args.lambda_h},alpha={args.alpha}, delta={args.delta}, c_lr={args.c_lr}, e_lr={args.e_lr} \")\nprint('f1:', round(np.mean(f1) * 100, 2), '±', round(np.std(f1) * 100, 2), sep='')\nprint('Auc:', round(np.mean(auc_roc) * 100, 2), '±', round(np.std(auc_roc) * 100, 2), sep='')\nprint('Acc:', round(np.mean(acc) * 100, 2), '±', round(np.std(acc) * 100, 2), sep='')\nprint('parity:', round(np.mean(parity) * 100, 2), '±', round(np.std(parity) * 100, 2), sep='')\nprint('equality:', round(np.mean(equality) * 100, 2), '±', round(np.std(equality) * 100, 2), sep='')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:44:20.623881Z","iopub.execute_input":"2026-01-05T08:44:20.624212Z","iopub.status.idle":"2026-01-05T08:49:50.234473Z","shell.execute_reply.started":"2026-01-05T08:44:20.624184Z","shell.execute_reply":"2026-01-05T08:49:50.233620Z"}},"outputs":[{"name":"stdout","text":"Credit 数据集特征： ['Married', 'EducationLevel', 'MaxBillAmountOverLast6Months', 'MaxPaymentAmountOverLast6Months', 'MonthsWithZeroBalanceOverLast6Months', 'MonthsWithLowSpendingOverLast6Months', 'MonthsWithHighSpendingOverLast6Months', 'MostRecentBillAmount', 'MostRecentPaymentAmount', 'TotalOverdueCounts', 'TotalMonthsOverdue', 'HistoryOfOverduePayments']\npredict_attr: Counter({1: 23364, 0: 6636})\nsens_attr: Counter({0: 27315, 1: 2685})\n===credit ===\nKMeans 预测准确率: 0.8707666666666667\nKMeans 伪s分布: Counter({np.int64(0): 28736, np.int64(1): 1264})\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [00:11<00:00,  2.27s/run]","output_type":"stream"},{"name":"stdout","text":"======credit======\nlambda_y=1,lambda_y=4,alpha=1, delta=0.5, c_lr=0.1, e_lr=0.1 \nf1:86.53±1.6\nAuc:71.12±2.15\nAcc:77.66±1.14\nparity:5.24±4.47\nequality:3.95±3.67\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27}]}